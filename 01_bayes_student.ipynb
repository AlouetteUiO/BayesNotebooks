{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' rule\n",
    "\n",
    "$ P( x | y ) = \\frac{P( y | x ) \\, P( x )}{ P(y) } $\n",
    "\n",
    "$ P(x) $ = prior belief of the state, before seeing any data\n",
    "\n",
    "$ P(y|x) $ = probability of seeing the data given the state\n",
    "\n",
    "$ P(y) $ = probability of seeing the data\n",
    "\n",
    "$ P(x|y) $ = posterior belief of the state, after seeing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our dilemma\n",
    "\n",
    "We are at a pingo site in Adventdalen. We want to know if the pingo is emitting methane or not. \n",
    "\n",
    "We start our experiment by assuming there is a 50/50 percent chance that the pingo is a methane source. \n",
    "\n",
    "What do we do? We measure the methane concentration downwind from the pingo. Suppose that we measure a methane concentration of 2.2 ppm. \n",
    "\n",
    "What do we (assume to) know from our observation model? \n",
    "- The probability of measuring a methane concentration above 2 ppm downwind from a pingo emitting methane is 70 percent.\n",
    "- The probability of measuring a methane concentration above 2 ppm downwind from a pingo NOT emitting methane is 20 percent.\n",
    "\n",
    "(These statistics are entirely made-up for this exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate this to math\n",
    "\n",
    "### Probability\n",
    "\n",
    "Uncertainty in the outcomes of a random variable $A$ is represented by a probability distribution. The probability of any particular outcome of the random variable, $A = a$, lies between 0 and 1, such that: \n",
    "$$ 0 \\le P(A=a) \\le 1$$\n",
    "\n",
    "In this exercise, we look at discrete probability distributions, which denote distributions over a discrete set of values. The distribution associated with random variable $A$ assigns probabilities to each possible value that $A$ can take. The probabilities sum to 1::\n",
    "\n",
    "$$ \\sum_{i=1}^n P(A = a_i) = 1 $$\n",
    "\n",
    "For our dilemma, the statement \"the pingo is a methane source\" is modeled as a binary random variable that can take on two values: true of false:\n",
    "\n",
    "$ P(source) = 0.5 $\n",
    "\n",
    "$ P( \\neg source) = 1 - P(source) = 0.5 $\n",
    "\n",
    "Our prior belief $P(x)$ is a discrete probability distribution over these two values: $ P(source)$ and $ P( \\neg source) $. As you can see, the discrete probability distribution of a binary random variable can be fully characterized by the probability of one state, $P(source)$, as the probability of the other state is simply $1-P(source)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_X = 0.5\n",
    "P_notX = 1 - P_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4,3))\n",
    "\n",
    "categories = ['source', 'not source']\n",
    "\n",
    "ax.bar(categories, [P_X, P_notX])\n",
    "ax.set_title('Prior')\n",
    "ax.set_ylabel('P(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probability\n",
    "\n",
    "The likelihood is given by conditional probability $ P(y|x) $.\n",
    "\n",
    "$ P(y|x) $ = probability of $y$, given $x$.\n",
    "\n",
    "For our dilemma, we have:\n",
    "\n",
    "$ P( > 2\\,ppm | source ) = 0.7 $\n",
    "\n",
    "$ P( > 2\\,ppm | \\neg source ) = 0.2 $\n",
    "\n",
    "Note that $ P(y|x) $ is generally NOT the same as $ P(x|y)$. For example, $P ( rain | cloudy ) \\neq P ( cloudy | rain )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Y_given_X = 0.7\n",
    "P_Y_given_notX = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint probability\n",
    "\n",
    "$ P( y \\cap x ) $ = probability that both $y$ and $x$ are the case. This is also written as $ P( y, x ) $.\n",
    "\n",
    "The product rule: $$ P( y \\cap x) = P (y|x) \\, P (x)$$\n",
    "\n",
    "For our dilemma:\n",
    "\n",
    "$ P( > 2\\,ppm \\cap source ) = P( > 2\\,ppm | source ) \\, P(source) = $ ? \n",
    "\n",
    "$ P( > 2\\,ppm \\cap \\neg source ) = P( > 2\\,ppm | \\neg source ) \\, P(\\neg source) = $ ? \n",
    "\n",
    "Note that $ P(x \\cap y) $ is the same as $ P(y \\cap x) $. For example, $P ( rain \\cap cloudy ) = P ( cloudy \\cap rain )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the expressionsss below\n",
    "P_Y_and_X = ...\n",
    "P_Y_and_notX = ...\n",
    "\n",
    "print(f\"P(>2 ppm and source) = {P_Y_and_X}\")\n",
    "print(f\"P(>2 ppm and not source) = {P_Y_and_notX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did Thomas Bayes notice?\n",
    "\n",
    "We have:\n",
    "\n",
    "$ P (y \\cap x) = P(y | x) \\, P(x) $\n",
    "\n",
    "$ P (x \\cap y) = P(x | y) \\, P(y) $\n",
    "\n",
    "Because $ P (y \\cap x) = P (x \\cap y) $:\n",
    "\n",
    "$ P(y | x) \\, P(x) = P(x | y) \\, P(y) $\n",
    "\n",
    "We can rewrite this as Bayes' rule:\n",
    "\n",
    "$$ P(x | y) = \\frac{P(y | x) \\, P(x)}{ P(y) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply this to our dilemma\n",
    "\n",
    "For our dilemma:\n",
    "\n",
    "$ P(source | > 2 \\, ppm ) = \\frac{ P(> 2\\,ppm | source ) \\, P( source) } {P( > 2\\, ppm)} $.\n",
    "\n",
    "We discussed the numerator, but not the denominator. What is $ P( > 2\\, ppm) $? \n",
    "\n",
    "Here, we use the sum rule. For discrete random variables: \n",
    "\n",
    "$$P(y) = \\sum_{i} P( y \\cap x_i) $$\n",
    "\n",
    "Such that (using joint probability):\n",
    "\n",
    "$$P(y) = \\sum_{i} P( y | x_i) \\, P (x_i) $$\n",
    "\n",
    "We can write Bayes' theorem as:\n",
    "\n",
    "$$ P(x | y ) = \\frac{ P( y | x ) \\, P( x ) } { \\sum_i P( y | x_i) \\, P (x_i) } $$\n",
    "\n",
    "In our dilemma, the discrete probability distribution over the state can only two values. We can write:\n",
    "\n",
    "$ P(source | > 2 \\, ppm ) = \\frac{ P(> 2\\,ppm | source ) \\, P( source) } {P( > 2\\, ppm \\, \\cap \\, source) + P( > 2\\, ppm \\, \\cap \\, \\neg source)} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the expression below\n",
    "P_X_given_Y = ...\n",
    "print(f\"P(source | >2 ppm) = {P_X_given_Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(6,3))\n",
    "\n",
    "categories = ['source', 'not source']\n",
    "\n",
    "ax[0].bar(categories, [P_X, P_notX])\n",
    "ax[0].set_title('Prior')\n",
    "ax[0].set_ylabel('P(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylim([0,1])\n",
    "\n",
    "ax[1].bar(categories, [P_X_given_Y, 1-P_Y_given_X])\n",
    "ax[1].set_title('Posterior')\n",
    "ax[1].set_ylabel('P(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple observations\n",
    "\n",
    "Suppose we obtain one more observation, such that we have $y_1$ and $y_2$. \n",
    "\n",
    "Bayes' rule for two observations:\n",
    "\n",
    "$$ P(x | y_1 \\cap y_2 ) = \\frac{P(y_1 \\cap y_2 | x) \\, P(x)}{ P(y_1 \\cap y_2) } $$\n",
    "\n",
    "We assume that the observations are conditionally independent given state $x$, such that:\n",
    "\n",
    "$$ P(y_1 \\cap y_2 | x ) = P(y_1 | x ) \\, P(y_2 | x ) $$\n",
    "\n",
    "We can then write:\n",
    "\n",
    "$$ P(x | y_1 \\cap y_2 ) = \\frac{P(y_1 | x ) \\, P(y_2 | x ) \\, P(x )}{ \\sum_i P(y_1|x_i) \\, P(y_2|x_i) \\, P(x_i) } $$\n",
    "\n",
    "Suppose that our second measurement gave 1.9 ppm. What is the batch (using both observations) posterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our first observation of 2.2 pm \n",
    "P_Y1_given_X = 0.7\n",
    "P_Y1_given_notX = 0.2\n",
    "\n",
    "# for our second observation of 1.9 ppm\n",
    "# fill in the probabilities\n",
    "P_Y2_given_X = ...\n",
    "P_Y2_given_notX = ...\n",
    "\n",
    "# complete the expression\n",
    "batch_posterior = ... \n",
    "\n",
    "print(f\"batch posterior = {batch_posterior}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(6,3))\n",
    "\n",
    "categories = ['source', 'not source']\n",
    "\n",
    "ax[0].bar(categories, [P_X, P_notX])\n",
    "ax[0].set_title('Prior')\n",
    "ax[0].set_ylabel('P(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylim([0,1])\n",
    "\n",
    "ax[1].bar(categories, [batch_posterior, 1-batch_posterior])\n",
    "ax[1].set_title('Batch posterior')\n",
    "ax[1].set_ylabel('P(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential updating\n",
    "\n",
    "Note that we can also update our belief sequentially. After applying Bayes' rule to the first observation, we use the resulting posterior as prior when we apply Bayes' rule to the second observation. \n",
    "\n",
    "1) Apply Bayes' rule with $y_1$, as follows: $ P(x | y_1 ) = \\frac{P(y_1 | x) \\, P(x)}{ P(y_1) } $.\n",
    "\n",
    "2) The posterior $ P(x | y_1 ) $ becomes the new prior $P (x)$.\n",
    "\n",
    "3) Apply Bayes' rule with $y_2$.\n",
    "\n",
    "Does this give the same final posterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first observation\n",
    "prior = P_X\n",
    "# complete the expression\n",
    "posterior_1obs = P_Y1_given_X * prior / ...\n",
    "\n",
    "# for the second observation, using the posterior of the first iteration as prior\n",
    "prior = posterior_1obs\n",
    "# complete the expression\n",
    "posterior_2obs = ...\n",
    "\n",
    "print(f\"final posterior = {posterior_2obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(9,3))\n",
    "\n",
    "categories = ['source', 'not source']\n",
    "\n",
    "ax[0].bar(categories, [P_X, P_notX])\n",
    "ax[0].set_title('Prior')\n",
    "ax[0].set_ylabel('P(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylim([0,1])\n",
    "\n",
    "ax[1].bar(categories, [posterior_1obs, 1-posterior_1obs])\n",
    "ax[1].set_title('Posterior after 1 obs')\n",
    "ax[1].set_ylabel('P(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylim([0,1])\n",
    "\n",
    "ax[2].bar(categories, [posterior_2obs, 1-posterior_2obs])\n",
    "ax[2].set_title('Posterior after 2 obs')\n",
    "ax[2].set_ylabel('P(x)')\n",
    "ax[2].set_xlabel('x')\n",
    "ax[2].set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of observations\n",
    "\n",
    "Does it matter in which order we process the observations? Do we get the same answer if we first update with $y_2$ and then with $y_1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the expressions\n",
    "\n",
    "prior = P_X\n",
    "posterior_1obs = P_Y2_given_X * prior / ...\n",
    "\n",
    "prior = posterior_1obs\n",
    "posterior_2obs = ...\n",
    "\n",
    "print(f\"final posterior = {posterior_2obs}\")\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(9,3))\n",
    "\n",
    "categories = ['source', 'not source']\n",
    "\n",
    "ax[0].bar(categories, [P_X, P_notX])\n",
    "ax[0].set_title('Prior')\n",
    "ax[0].set_ylabel('P(x)')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylim([0,1])\n",
    "\n",
    "ax[1].bar(categories, [posterior_1obs, 1-posterior_1obs])\n",
    "ax[1].set_title('Posterior after 1 obs')\n",
    "ax[1].set_ylabel('P(x)')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylim([0,1])\n",
    "\n",
    "ax[2].bar(categories, [posterior_2obs, 1-posterior_2obs])\n",
    "ax[2].set_title('Posterior after 2 obs')\n",
    "ax[2].set_ylabel('P(x)')\n",
    "ax[2].set_xlabel('x')\n",
    "ax[2].set_ylim([0,1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denominator as normalization constant\n",
    "\n",
    "Note that the denominator in Bayes' rule acts as a normalizing constant. It is used to scale the posterior. \n",
    "\n",
    "Bayes' theorem is therefore often given by:\n",
    "\n",
    "$$ P(x | y ) \\propto P (y | x) \\, P(x) $$\n",
    "\n",
    "This can help us with coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, to compute the entire distribution:\n",
    "\n",
    "prior = np.array([P_X, P_notX])\n",
    "likelihood = np.array([P_Y_given_X, P_Y_given_notX])\n",
    "posterior = prior * likelihood\n",
    "posterior /= np.sum(posterior)\n",
    "print(f\"posterior distribution = {posterior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another dilemma\n",
    "\n",
    "In the previous exercise, the state could take two possible values, represented by a discrete probability distribution over these two outcomes. We will now look at a more complex dilemma where the probability mass is distributed across five possible values. \n",
    "\n",
    "Our dilemma:\n",
    "\n",
    "A methane source emits particles, and our goal is to estimate the number of particles emitted by the source per unit time. We know that the emission rate can be 1, 2, 3, 4 or 5 particles per unit time. Our belief regarding the emission rate is represented by a probability distribution over these five values.\n",
    "\n",
    "We observe the methane concentration downwind from the methane source. The observation is given as the detected number of particles within a fixed sampling time.\n",
    "\n",
    "For simplicity, we use a strightforward, entirely made-up, observational model. The expected number of particles detected within the sampling time is equal to the number of particles emitted by the source per unit time. However, our observations are noisy. We model this using a Poisson distribution for the probability mass function of the detected number of particles within the sample time:\n",
    "$$ P(k|\\lambda) = \\frac{\\lambda^k}{k!} \\exp(- \\lambda) $$\n",
    "where:\n",
    "- $k$ is the number of occurances, in this context,, it is the number of particles detected during the sampling time: $k = y$.\n",
    "- $\\lambda$ is the expected value of $k$. Here, it is the expected number of particles detected during the sample time. Since we have assumed for simplicity that this equals the number of particles emitted by the source per unit time, $\\lambda = x$. (In a more complex exercise, $\\lambda$ could follow from an atmospheric dispersion model.) \n",
    "\n",
    "The likelihood $p(y|x)$ based on this observational model can then be expressed as using the Poisson distribution.  \n",
    "\n",
    "Consider different potential measurements and a prior belief (this does not need to be a uniform distribution). What is the resulting posterior belief? Plot the prior and posterior beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(y, x):\n",
    "    \"\"\"\n",
    "    The probability of observing y given that x is true.\n",
    "    y: observation [number of detected particles in sampling time]\n",
    "    x: state [number of particles emitted per unit time]\n",
    "    \"\"\"\n",
    "\n",
    "    # factorial of y\n",
    "    fact = 1  \n",
    "    for i in range(2, y+1):\n",
    "        fact *= i\n",
    "\n",
    "    # Poisson distribution\n",
    "    return (x**y * np.exp(-x)) / fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some observations, a prior, and compute the posterior. \n",
    "# you can use the set-up below for this (for sequential Bayesian inference), \n",
    "# feel free to adjust it, or do it in a completely different way!\n",
    "\n",
    "# define the observations and prior are numpy arrays\n",
    "observations = np.array([ ... ])\n",
    "prior = ...\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "x = np.arange(5)\n",
    "width = 0.1  # the width of the bars\n",
    "multiplier = 1\n",
    "\n",
    "rects = ax.bar(x, prior, width, label='prior')\n",
    "\n",
    "for i, obs in enumerate(observations):\n",
    "\n",
    "    # compute the posterior \n",
    "    ...\n",
    "    \n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x+offset, posterior, width, label=f'after {i+1} obs')\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('P(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xticks(x + width, ['1', '2', '3', '4', '5'])\n",
    "ax.set_ylim([0,1])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cromwell's rule\n",
    "\n",
    "Cromwell's rule (Lindley, 2013) states that:\n",
    "\n",
    "If $P(x) = 0$ then $P(x|y)=0$. If $P(x)=1$ and $P(y)>0$, then $P(x|y) = 1$.\n",
    "\n",
    "Or in words, Cromwell's rule says that if the prior probability assigned to a random variable is 0 or 1, then according to Bayes' theorem, the posterior probability is forced to be 0 or 1 as well. No evidence, no matter how strong, could have any influence. So, hard convictions are insensitive to counter-evidence. \n",
    "\n",
    "You can test this rule for our dilemma above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some links to YouTube movies about Bayes' rule:\n",
    "\n",
    "https://www.youtube.com/watch?v=5NMxiOGL39M&t=22s\n",
    "\n",
    "https://www.youtube.com/watch?v=HZGCoVF3YvM&t=261s\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

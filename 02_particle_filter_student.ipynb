{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our dilemma\n",
    "\n",
    "In this exercise, we will look at flux-profile inversion. Our goal is to infer the friction velocity $u_{*} = \\sqrt{- \\overline{u'w'}}$ and roughness length $z_0$ using drone measurements of the mean horizontal wind speed $U(z)$ profile. \n",
    "\n",
    "For this, we will use the 'law of the wall':\n",
    "$$ U(u_{*}, z_0, z) = \\frac{u_{*}}{\\kappa} \\log(z/z_0) $$\n",
    "which describes how the mean horizontal wind speed $U$ increases logarithmically with height $z$ in the neutral atmospheric surface layer. We want to infer the friction velocity $u_{*} = \\sqrt{- \\overline{u'w'}}$ and roughness length $z_0$ given drone measurements of the mean horizontal wind speed $U$ at various heights $z$. $\\kappa = 0.4$ is the von KÃ¡rman constant. \n",
    "\n",
    "We can express our inverse problem as follows:\n",
    "$$ y = \\mathcal{F}(\\mathbf{x}, z) + \\epsilon $$\n",
    "where $\\epsilon$ is the noise term representing measurement error. \n",
    "\n",
    "What are $y$, $\\mathbf{x}$, and $\\mathcal{F}$? \n",
    "\n",
    "We will use the particle filter to infer the unknown parameters in a synthetic experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mapping(u_s, z_0, z):\n",
    "    \"\"\"\n",
    "    'law of the wall' describing idealized mean horizontal wind profiles\n",
    "    in the atmospheric surface layer under neutral stability conditions.\n",
    "\n",
    "    u_s:    friction velocity [m/s]\n",
    "    z_0:    roughness length [m]\n",
    "    z:      alititude [m]\n",
    "    kappa:  von Karman constant [-]\n",
    "    \"\"\"\n",
    "    kappa = 0.4 \n",
    "    U = (u_s/kappa) * np.log(z/z_0) \n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate our \"thruth\" for this synthetic experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our \"thruth\"\n",
    "u_s_true = 0.5\n",
    "z_0_true = 0.1\n",
    "\n",
    "# vizualize the true mean horizontal wind speed profile\n",
    "N_z = 5\n",
    "z = np.logspace(0, 2, N_z) # n_z level evenly spaced on a log scale from 1 to 100 m\n",
    "U_true = forward_mapping(u_s_true, z_0_true, z)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(U_true, z, 'k')\n",
    "ax.set_title('True wind speed profile')\n",
    "ax.set_xlabel('Horizontal wind $U$ [m/s]')\n",
    "ax.set_ylabel('Altitude $z$ [m]')\n",
    "ax.set_ylim(0, 110)\n",
    "ax.set_xlim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create some synthetic data to mimic real, noisy, observations. We add some observation error to the true wind speed profile. We model the noise as a normal distribution with mean $\\mu = 0$ and variance $\\sigma^2 = \\sigma^2_y$. \n",
    "\n",
    "We have:\n",
    "$$ y_i = \\mathcal{F}(\\mathbf{x}, z_i) + \\epsilon_i $$\n",
    "where the additive Gaussian noise has the form $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_y^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng() # create random number generator object\n",
    "\n",
    "# fill in the standard deviation of the Gaussian noise\n",
    "U_sigma = ...\n",
    "U_obs = rng.normal(loc=U_true, scale=U_sigma, size=N_z)\n",
    "U_obs = np.maximum(U_obs, 0) # Truncate to avoid negative wind speed observations.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(U_true, z, 'k', label='Truth')\n",
    "ax.scatter(U_obs, z, 50, color=(0.8,0.4,0), alpha=0.8, label='Obs')\n",
    "ax.set_title('True wind speed profile and observations')\n",
    "ax.set_xlabel('Horizontal wind $U$ [m/s]')\n",
    "ax.set_ylabel('Altitude $z$ [m]')\n",
    "ax.set_ylim(0, 110)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle filter/smoother\n",
    "\n",
    "We assume that we do not know the true windspeed profile, and we use our noisy observations and a particle filter to infer the friction velocity and roughness length. \n",
    "\n",
    "We start with defining a prior. Remember that all the particles are considered to have the same weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the number of particles\n",
    "# generally, more particles gives more accuracy, but also larger computational time. \n",
    "N_particles = int(...)  \n",
    "\n",
    "# fill in the bounds of the uniform prior\n",
    "u_s_min, u_s_max = ..., ...\n",
    "z_0_min, z_0_max = ..., ...\n",
    "\n",
    "u_s_prior = rng.uniform(low=u_s_min, high=u_s_max, size=N_particles)\n",
    "z_0_prior = rng.uniform(low=z_0_min, high=z_0_max, size=N_particles)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(u_s_prior, density=True)  # with density=True, the function returns a probability density, the area under the histogram integrates to 1.\n",
    "ax[0].set_ylabel('Probability density')\n",
    "ax[0].set_xlabel('$u_s$ [m/s]')\n",
    "ax[0].set_xlim((u_s_min, u_s_max))\n",
    "ax[1].hist(z_0_prior, density=True)\n",
    "ax[1].set_ylabel('Probability density')\n",
    "ax[1].set_xlabel('$z_0$ [m]')\n",
    "ax[1].set_xlim((z_0_min, z_0_max))\n",
    "fig.suptitle('Priors')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the prior and our forward mapping to get prior predictions of the mean wind speed profile. \n",
    "\n",
    "$$\\hat{y} = \\mathcal{F}(\\mathbf{x}, z) $$\n",
    "\n",
    "where $\\hat{y}$ are the prior predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_prediction(u_s_prior, z_0_prior, z):\n",
    "    u_s_prior = np.broadcast_to(u_s_prior, (N_z, N_particles)) \n",
    "    z_0_prior = np.broadcast_to(z_0_prior, (N_z, N_particles))\n",
    "    z = np.broadcast_to(z[:, np.newaxis], (N_z, N_particles))\n",
    "    U_prior_prediction = forward_mapping(u_s_prior, z_0_prior, z)\n",
    "    return U_prior_prediction\n",
    "\n",
    "U_prior_prediction = get_prior_prediction(u_s_prior, z_0_prior, z)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(U_prior_prediction, z, alpha=0.4)\n",
    "ax.set_xlabel('Horizontal wind U [m/s]')\n",
    "ax.set_ylabel('Altitude z [m]')\n",
    "ax.set_title('Prior predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particle filter/smoother uses importance sampling. We apply the bootstrap filter, whereby the proposal distribution is our prior. The weights can then be obtained through a simple likelihood ratio. The weight of the $j^{th}$ particle is given by:\n",
    "\n",
    "$$ w_j = \\frac{ p({\\mathbf{y}}|\\mathbf{x}_j) }{ \\sum_{k=1}^{N_e} p(\\mathbf{y}|\\mathbf{x}_k )} $$\n",
    "\n",
    "where $N_e$ is the number of particles and $\\mathbf{y}$ is the batch of $N$ observations.\n",
    "\n",
    "Modeling the measurement noise as a Gaussian distribution, leads to the following Gaussian likelihood:\n",
    "\n",
    "$$ p(y_i | \\mathbf{x}, z_i ) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp \\left( - \\frac{(y_i - \\hat{y}_i)^2}{2 \\sigma_y^2} \\right)  $$\n",
    "\n",
    "The weights can be computed by:\n",
    "\n",
    "$$ w_j = \\frac{ \\exp \\left( - \\sum_{i=1}^{N} \\frac{(y_i - \\mathcal{F}(\\mathbf{x}_j, z_i))^2}{2 \\sigma_y^2} \\right) }{ \\sum_{k=1}^{N_e} \\exp \\left( - \\sum_{i=1}^{N} \\frac{(y_i - \\mathcal{F}(\\mathbf{x}_k, z_i))^2}{2 \\sigma_y^2} \\right) } $$\n",
    "\n",
    "To ensure numerical stability, we first calculate the logarithm of the weigths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sampling(y, y_hat, y_sigma):\n",
    "    y = np.broadcast_to(y[:, np.newaxis], (N_z, N_particles))\n",
    "    error = y - y_hat\n",
    "    log_likelihood = -0.5 * np.sum(error**2, axis=0) / (y_sigma**2) \n",
    "    denominator = np.log( np.sum( np.exp( log_likelihood) ) )  # in scipy, you can use the function logsumexp\n",
    "    log_weights = log_likelihood - denominator\n",
    "    weights = np.exp(log_weights)\n",
    "    weights = weights / np.sum(weights)  # to ensure normalized weights\n",
    "    return weights\n",
    "\n",
    "weights = importance_sampling(U_obs, U_prior_prediction, U_sigma)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(u_s_prior, density=True, weights=weights)  # plot the same particles but now with updated weights\n",
    "ax[0].set_ylabel('Probability density')\n",
    "ax[0].set_xlabel('$u_s$ [m/s]')\n",
    "ax[0].set_xlim((u_s_min, u_s_max))\n",
    "ax[1].hist(z_0_prior, density=True, weights=weights)\n",
    "ax[1].set_ylabel('Probability density')\n",
    "ax[1].set_xlabel('$z_0$ [m]')\n",
    "ax[1].set_xlim((z_0_min, z_0_max))\n",
    "fig.suptitle('Weighted distributions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_s_mean = np.sum(weights * u_s_prior)\n",
    "z_0_mean = np.sum(weights * z_0_prior)\n",
    "\n",
    "u_s_sigma = np.sqrt(np.sum(weights * (u_s_prior - u_s_mean)**2))\n",
    "z_0_sigma = np.sqrt(np.sum(weights * (z_0_prior - z_0_mean)**2))\n",
    "\n",
    "print(f\"posterior mean u_s = {u_s_mean} m/s\")\n",
    "print(f\"posterior sigma u_s = {u_s_sigma} m/s\")\n",
    "\n",
    "print(f\"posterior mean z_0 = {z_0_mean} m\")\n",
    "print(f\"posterior sigma z_0 = {z_0_sigma} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective sample size \n",
    "\n",
    "The effective sample size is a metric for the accuracy of the estimates. In the ideal case we have that $w_j = 1/N_e$ for all particles $j$ so that ESS = $N_e$. In the worst case, the so-called degenerate case, all the weight is on one particle so that ESS = 1. \n",
    "\n",
    "$$ ESS = \\frac{1}{\\sum_j w_j^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ESS(weights):\n",
    "    N_eff =1/np.sum(weights**2)\n",
    "    return N_eff\n",
    "\n",
    "N_eff = get_ESS(weights)\n",
    "\n",
    "print(f\"ESS = {N_eff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "\n",
    "It is possible to resample the particles based on their weight. Hereby we copy high weight particles and remove the low weight particles. In this way, we end up with equally weighted resampled particles. \n",
    "\n",
    "We can then:\n",
    "- Use simple arithmetic (instead of weighted) means,\n",
    "- We can use the resampled particles as the prior for a (possible) next iteration of the particle filter with new data. If the ESS is low, it is common to add some 'jitter' to the particles, so-called rejuvenation. This is to prevent the collapse of the weights. You just have to make sure that the distribution remains within the bounds of the initial prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling\n",
    "selection_index = rng.choice(N_particles, size=N_particles, p=weights)  # replace = True by default\n",
    "u_s_resampled = u_s_prior[selection_index]\n",
    "z_0_resampled = z_0_prior[selection_index]\n",
    "\n",
    "# rejuvenation\n",
    "u_s_new_prior = rng.normal(loc=u_s_resampled, scale=0.01)  # scale is the standard deviation of the Gaussian added noise\n",
    "u_s_new_prior = np.maximum(u_s_min, np.minimum(u_s_new_prior, np.ones(N_particles)*u_s_max))  # make sure that the distribution stays within bounds\n",
    "z_0_new_prior = rng.normal(loc=z_0_resampled, scale=0.01)  \n",
    "z_0_new_prior = np.maximum(np.ones(N_particles)*z_0_min, np.minimum(z_0_new_prior, np.ones(N_particles)*z_0_max))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(u_s_resampled, density=True)  \n",
    "ax[0].set_ylabel('Probability density')\n",
    "ax[0].set_xlabel('$u_s$ [m/s]')\n",
    "ax[0].set_xlim((u_s_min, u_s_max))\n",
    "ax[1].hist(z_0_resampled, density=True)\n",
    "ax[1].set_ylabel('Probability density')\n",
    "ax[1].set_xlabel('$z_0$ [m]')\n",
    "ax[1].set_xlim((z_0_min, z_0_max))\n",
    "fig.suptitle('Resampled distributions')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(u_s_new_prior, density=True)  \n",
    "ax[0].set_ylabel('Probability density')\n",
    "ax[0].set_xlabel('$u_s$ [m/s]')\n",
    "ax[0].set_xlim((u_s_min, u_s_max))\n",
    "ax[1].hist(z_0_new_prior, density=True)\n",
    "ax[1].set_ylabel('Probability density')\n",
    "ax[1].set_xlabel('$z_0$ [m]')\n",
    "ax[1].set_xlim((z_0_min, z_0_max))\n",
    "fig.suptitle('Rejuvenated distributions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create another set of synthetic observations and run the particle filter with this new set. Can we further constrain the probability distribution? Do we approach the thruth? What is the ESS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another set of wind speed profile observations and apply the particle filter. \n",
    "# You can use the posterior of the previous iteration as prior, or come up with a different prior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
